{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quentin-arzalier/tp-information-retrieval-with-llm-student-version/blob/main/1-Recherche%20d'information%20classique.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugX7FayQ90_A"
      },
      "source": [
        "# Partie 1. - Recherche d'Information classique\n",
        "\n",
        "Dans cette partie, nous allons mettre en oeuvre des principes et modèles classiques de Recherche d'Information. Le jeu de donnée est une collection de livres au format texte (.txt) de Henry Rider Haggard. Jetez un oeil à ces documents dans le dossier _data_.\n",
        "\n",
        "En sortie de ce module, vous serez capable de :\n",
        "\n",
        "- Construire un index inversé ;\n",
        "- Effectuer des requêtes simples selon le modèle booléen :\n",
        "- Calculer la pondération des termes selon la méthode TF-IDF ;\n",
        "- Mettre en oeuvre le modèle vectoriel de recherche d'information et y appliquer des requêtes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw1BGZgY-tvZ"
      },
      "source": [
        "### Import des bibliothèques logicielles et configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOeUX1DuBqQL"
      },
      "source": [
        "Les lignes suivantes permettent d'instancier un objet la classe `IRSystem` représentant notre moteur de recherche et de charger les données en RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-oi-56kBqQL",
        "outputId": "14103c32-748f-4ce1-b26a-6060a0d58202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tp-information-retrieval-with-llm-student-version'...\n",
            "remote: Enumerating objects: 2241, done.\u001b[K\n",
            "remote: Counting objects: 100% (2241/2241), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2228/2228), done.\u001b[K\n",
            "remote: Total 2241 (delta 22), reused 2227 (delta 12), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (2241/2241), 13.91 MiB | 4.33 MiB/s, done.\n",
            "Resolving deltas: 100% (22/22), done.\n",
            "/content/tp-information-retrieval-with-llm-student-version\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Vérifie si le code est exécuté sur Google Colab\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "    # Commandes à exécuter uniquement sur Google Colab\n",
        "    !git clone https://github.com/quentin-arzalier/tp-information-retrieval-with-llm-student-version.git\n",
        "    %cd tp-information-retrieval-with-llm-student-version\n",
        "else:\n",
        "    # Commandes à exécuter si ce n'est pas sur Google Colab\n",
        "    print(\"Pas sur Google Colab, ces commandes ne seront pas exécutées.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rMp8JVGBqQM"
      },
      "source": [
        "#### Chargement des données\n",
        "\n",
        "Les lignes ci-dessous permettent de charger les données qui sont un ensemble de 60 livres au format texte (.txt) d'[Henry Rider Haggard ](https://fr.wikipedia.org/wiki/Henry_Rider_Haggard).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZNWPECSs9wgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a21b989-b7e7-4bee-dabf-81779e22283c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading in documents...\n",
            "Stemming Documents...\n",
            "The Tale of Three Lions 2729.txt\n",
            "    Doc 1 of 60: The Tale of Three Lions\n",
            "The Brethren 2762.txt\n",
            "    Doc 2 of 60: The Brethren\n",
            "Fair Margaret 9780.txt\n",
            "    Doc 3 of 60: Fair Margaret\n",
            "The Ghost Kings 8184.txt\n",
            "    Doc 4 of 60: The Ghost Kings\n",
            "Allan Quatermain 711.txt\n",
            "    Doc 5 of 60: Allan Quatermain\n",
            "Finished 1724.txt\n",
            "    Doc 6 of 60: Finished\n",
            "A Winter Pilgrimage (1901) 0600121.txt\n",
            "    Doc 7 of 60: A Winter Pilgrimage (1901)\n",
            "The Ancient Allan 5746.txt\n",
            "    Doc 8 of 60: The Ancient Allan\n",
            "She and Allan 5745.txt\n",
            "    Doc 9 of 60: She and Allan\n",
            "Allan and the Ice Gods (1927) 0200201.txt\n",
            "    Doc 10 of 60: Allan and the Ice Gods (1927)\n",
            "Black Heart and White Heart 2842.txt\n",
            "    Doc 11 of 60: Black Heart and White Heart\n",
            "Cetywayo and his White Neighbours 0.txt\n",
            "    Doc 12 of 60: Cetywayo and his White Neighbours\n",
            "A Yellow God an Idol of Africa 2857.txt\n",
            "    Doc 13 of 60: A Yellow God an Idol of Africa\n",
            "Smith and the Pharaohs, and other Tales 6073.txt\n",
            "    Doc 14 of 60: Smith and the Pharaohs, and other Tales\n",
            "The Witch's Head (1884) 0500791.txt\n",
            "    Doc 15 of 60: The Witch's Head (1884)\n",
            "The Mahatma and the Hare 2764.txt\n",
            "    Doc 16 of 60: The Mahatma and the Hare\n",
            "The Wizard 2893.txt\n",
            "    Doc 17 of 60: The Wizard\n",
            "Queen of the Dawn (1925) 0200381.txt\n",
            "    Doc 18 of 60: Queen of the Dawn (1925)\n",
            "Montezuma's Daughter 1848.txt\n",
            "    Doc 19 of 60: Montezuma's Daughter\n",
            "King Solomon's Mines 2166.txt\n",
            "    Doc 20 of 60: King Solomon's Mines\n",
            "Wisdom's Daughter (1923) 0200181.txt\n",
            "    Doc 21 of 60: Wisdom's Daughter (1923)\n",
            "Love Eternal 3709.txt\n",
            "    Doc 22 of 60: Love Eternal\n",
            "Mr. Meeson's Will 11913.txt\n",
            "    Doc 23 of 60: Mr. Meeson's Will\n",
            "Doctor Therne 5764.txt\n",
            "    Doc 24 of 60: Doctor Therne\n",
            "When the World Shook 0.txt\n",
            "    Doc 25 of 60: When the World Shook\n",
            "Stories by English Authors Africa (Selected by Scribners) 1980.txt\n",
            "    Doc 26 of 60: Stories by English Authors Africa (Selected by Scribners)\n",
            "Dawn 10892.txt\n",
            "    Doc 27 of 60: Dawn\n",
            "Colonel Quaritch, V.C. A Tale of Country Life 11882.txt\n",
            "    Doc 28 of 60: Colonel Quaritch, V.C. A Tale of Country Life\n",
            "Moon of Israel 2856.txt\n",
            "    Doc 29 of 60: Moon of Israel\n",
            "The People of the Mist 6769.txt\n",
            "    Doc 30 of 60: The People of the Mist\n",
            "Cleopatra 2769.txt\n",
            "    Doc 31 of 60: Cleopatra\n",
            "The World's Desire 2763.txt\n",
            "    Doc 32 of 60: The World's Desire\n",
            "Allan's Wife 2727.txt\n",
            "    Doc 33 of 60: Allan's Wife\n",
            "Regeneration 13434.txt\n",
            "    Doc 34 of 60: Regeneration\n",
            "The Lady of Blossholme 3813.txt\n",
            "    Doc 35 of 60: The Lady of Blossholme\n",
            "Morning Star 2722.txt\n",
            "    Doc 36 of 60: Morning Star\n",
            "Ayesha, the Return of She 5228.txt\n",
            "    Doc 37 of 60: Ayesha, the Return of She\n",
            "Child of Storm 1711.txt\n",
            "    Doc 38 of 60: Child of Storm\n",
            "Beatrice 3096.txt\n",
            "    Doc 39 of 60: Beatrice\n",
            "Eric Brighteyes 2721.txt\n",
            "    Doc 40 of 60: Eric Brighteyes\n",
            "Pearl-Maiden 5175.txt\n",
            "    Doc 41 of 60: Pearl-Maiden\n",
            "The Virgin of the Sun 3153.txt\n",
            "    Doc 42 of 60: The Virgin of the Sun\n",
            "Benita, an African romance 2761.txt\n",
            "    Doc 43 of 60: Benita, an African romance\n",
            "She 3155.txt\n",
            "    Doc 44 of 60: She\n",
            "Heu-Heu (1924) 0200191.txt\n",
            "    Doc 45 of 60: Heu-Heu (1924)\n",
            "Queen Sheba's Ring 2602.txt\n",
            "    Doc 46 of 60: Queen Sheba's Ring\n",
            "Hunter Quatermain's Story 2728.txt\n",
            "    Doc 47 of 60: Hunter Quatermain's Story\n",
            "Stella Fregelius 6051.txt\n",
            "    Doc 48 of 60: Stella Fregelius\n",
            "Red Eve 3094.txt\n",
            "    Doc 49 of 60: Red Eve\n",
            "Swallow a tale of the great trek 4074.txt\n",
            "    Doc 50 of 60: Swallow a tale of the great trek\n",
            "The Wanderer's Necklace 3097.txt\n",
            "    Doc 51 of 60: The Wanderer's Necklace\n",
            "Lysbeth, a Tale of the Dutch 5754.txt\n",
            "    Doc 52 of 60: Lysbeth, a Tale of the Dutch\n",
            "Nada the Lily 1207.txt\n",
            "    Doc 53 of 60: Nada the Lily\n",
            "Jess 5898.txt\n",
            "    Doc 54 of 60: Jess\n",
            "The Ivory Child 2841.txt\n",
            "    Doc 55 of 60: The Ivory Child\n",
            "Long Odds 1918.txt\n",
            "    Doc 56 of 60: Long Odds\n",
            "Maiwa's Revenge 2713.txt\n",
            "    Doc 57 of 60: Maiwa's Revenge\n",
            "Marie An Episode in The Life of the late Allan Quatermain 1690.txt\n",
            "    Doc 58 of 60: Marie An Episode in The Life of the late Allan Quatermain\n",
            "Allan and the Holy Flower 5174.txt\n",
            "    Doc 59 of 60: Allan and the Holy Flower\n",
            "Elissa 2855.txt\n",
            "    Doc 60 of 60: Elissa\n"
          ]
        }
      ],
      "source": [
        "from classic_ir.IRSystem import *\n",
        "\n",
        "# !rm -rf ./data/RiderHaggard/stemmed\n",
        "ir_system = IRSystem()\n",
        "ir_system.read_data('./data/RiderHaggard') # chargement des données et prétraitement des documents (stemming)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4n3kuGaBqQN"
      },
      "source": [
        "### Exercice 1. - Construction de l'index inversé\n",
        "\n",
        "Ce premier exercice a pour objectif de construire l'index inversé non positionnel. L'attribut `self.inverted_index` est un tableau associatif qui associe une liste d'entiers (docIDs) à un mot (word).\n",
        "\n",
        "Documentation ici https://docs.python.org/3/library/collections.html#collections.defaultdict.\n",
        "\n",
        "Exercice : modifier la fonction `index` pour calculer l'index inversé.\n",
        "\n",
        "Le résultat ci-dessous indique que vous avez réussi.\n",
        "```\n",
        "===== Running tests =====\n",
        "Inverted Index Test\n",
        "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzLzgP0SBqQO",
        "outputId": "a9f3a895-7edf-4c2f-f4c6-b410e3443538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexing...\n",
            "===== Running tests =====\n",
            "Inverted Index Test\n",
            "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n"
          ]
        }
      ],
      "source": [
        "# Exercice 1. Indexation\n",
        "\n",
        "def index(self):\n",
        "    \"\"\"\n",
        "    Construit l'index inversé et le stocke dans self.inverted_index.\n",
        "\n",
        "    Dans le code ci-dessous, seul le dictionnaire des tokens est construit. Les postings lists sont vides.\n",
        "    \"\"\"\n",
        "    print(\"Indexing...\")\n",
        "    self.tf = defaultdict(Counter) # tf est un dictionnaire qui à un document associe un Counter de mots.\n",
        "    inverted_index = defaultdict(list) # inverted_index est un dictionnaire qui à un mot associe une liste de documents.\n",
        "    for i in range (len(self.docs)):\n",
        "        for word in self.docs[i]:\n",
        "            if (not i in inverted_index[word]):\n",
        "              inverted_index[word].append(i)\n",
        "\n",
        "    self.inverted_index = inverted_index\n",
        "\n",
        "# Ne pas modifier les lignes ci-dessous\n",
        "IRSystem.index = index\n",
        "ir_system.index()\n",
        "run_tests(ir_system, part=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiskwodfBqQO"
      },
      "source": [
        "### Exercice 2. - Recherche booléenne\n",
        "\n",
        "Ce deuxième exercice a pour objectif d'implémenter la recherche booléenne. La requête `query` est une liste de mots _lemmatisés_ et l'algorithme doit rechercher les documents qui contiennent TOUS ces mots.\n",
        "\n",
        "\n",
        "Exercice : modifier la fonction `boolean_retrieve` pour implémenter la recherche booléenne.\n",
        "\n",
        "\n",
        "Le résultat ci-dessous indique que vous avez réussi.\n",
        "```\n",
        "===== Running tests =====\n",
        "Boolean Retrieval Test\n",
        "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aB9EXqMWBqQO"
      },
      "outputs": [],
      "source": [
        "# Exercice 2. Recherche booléenne\n",
        "def boolean_retrieve(self, query):\n",
        "    print(len(self.vocab))\n",
        "    \"\"\"\n",
        "    A partir d'une requête sous la forme d'une liste de mots *lemmatisés*,\n",
        "    retourne la liste des documents dans lesquels *tous* ces mots apparaissent (ie une requête AND).\n",
        "    Retourne une liste vide si la requête ne retourne aucun document.\n",
        "\n",
        "    Dans le code ci-dessous, tous les documents répondent.\n",
        "    \"\"\"\n",
        "    docs = list()\n",
        "    for idx, doc in enumerate(self.docs):\n",
        "        test=True\n",
        "        i=0\n",
        "        while(test and i < len(query)):\n",
        "          test=(query[i] in doc)\n",
        "          i+=1\n",
        "        if (test):\n",
        "          docs.append(idx)\n",
        "    return docs\n",
        "\n",
        "# Ne pas modifier les lignes ci-dessous\n",
        "IRSystem.boolean_retrieve = boolean_retrieve\n",
        "run_tests(ir_system, part=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2YjuO6NBqQP"
      },
      "source": [
        "### Exercice 3. - Calcul des poids TF-IDF des termes dans les documents\n",
        "\n",
        "Dans ce troisième exercice, l'objectif est de pré-calculer les poids TF-IDF pour chaque terme dans chaque document. Pour ce faire, appliquer la formule vue en cours. Utiliser le logarithme en base 10.\n",
        "\n",
        "\n",
        "Exercice : modifier la fonction `boolean_retrieve` pour implémenter la recherche booléenne.\n",
        "\n",
        "Le résultat ci-dessous indique que vous avez réussi.\n",
        "```\n",
        "Calculating tf-idf...\n",
        "===== Running tests =====\n",
        "TF-IDF Test\n",
        "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vhp6Tm2BqQP",
        "outputId": "70199692-ab75-41fa-d191-6832bc05e0c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating tf-idf...\n",
            "0.13337736503241182\n",
            "===== Running tests =====\n",
            "TF-IDF Test\n",
            "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n"
          ]
        }
      ],
      "source": [
        "from math import log10\n",
        "\n",
        "# Exercice 3. calcul des scores tf-idf\n",
        "def compute_tfidf(self):\n",
        "    \"\"\"\n",
        "    Calcule les scores tf-idf pour tous les mots de tous les documents et les stocke dans self.tfidf.\n",
        "\n",
        "    Dans le code ci-dessous, les scores tf-idf sont tous nuls.\n",
        "    \"\"\"\n",
        "    print(\"Calculating tf-idf...\")\n",
        "\n",
        "    N = len(self.docs)  # N = nombre de documents\n",
        "\n",
        "    occurences = defaultdict(Counter)\n",
        "    for i in range(N):\n",
        "      for word in self.docs[i]:\n",
        "        occurences[i][word] += 1\n",
        "\n",
        "\n",
        "    self.tfidf = defaultdict(Counter)\n",
        "    for i in range(N):\n",
        "      for idx, word in enumerate(self.docs[i]):\n",
        "        df = len(self.inverted_index[word])\n",
        "        idf = log10(N/df)\n",
        "        try:\n",
        "            self.tfidf[i][word] = (1+log10(occurences[i][word]))*idf\n",
        "        except ValueError:\n",
        "            self.tfidf[i][word] = 0.\n",
        "\n",
        "    print(self.tfidf[0][self.p.stem(\"separation\")])\n",
        "\n",
        "# Ne pas modifier les lignes ci-dessous\n",
        "IRSystem.compute_tfidf = compute_tfidf\n",
        "ir_system.compute_tfidf()\n",
        "run_tests(ir_system, part=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_N4kqqLBqQP"
      },
      "source": [
        "### Exercice 4. - Calcul de la similarité cosinus.\n",
        "\n",
        "Dans ce troisième exercice, l'objectif est de pré-calculer les poids TF-IDF pour chaque terme dans chaque document. Pour ce faire, appliquer la formule vue en cours en considérant les éléments suivants :\n",
        "- Ne considérer que la mesure TF pour calculer le poids des termes de la requête (on utilise pas IDF).\n",
        "- utiliser le logarithme en base 10.\n",
        "\n",
        "Exercice : modifier la fonction `rank_retrieve` pour implémenter la recherche booléenne.\n",
        "\n",
        "Le résultat ci-dessous indique que vous avez réussi.\n",
        "```\n",
        "===== Running tests =====\n",
        "Cosine Similarity Test\n",
        "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2MLgta7BqQQ",
        "outputId": "26a971f2-a28a-4ee1-fd61-e327952f6b22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Running tests =====\n",
            "Cosine Similarity Test\n",
            "['separ', 'of', 'church', 'and', 'state'] [(21, 0.0011901031212691925), (24, 0.0010427638051385489), (52, 0.0007314823072292831), (50, 0.00041515975534080945), (9, 0.00040267045155040275), (27, 0.00033483234970602696), (15, 0.00028358469515401077), (5, 0.00027059730265736285), (38, 0.0002703262561123598), (16, 0.00023049463957970225)]\n",
            "['priestess', 'ritual', 'sacrific'] [(16, 0.00010003497910178631), (59, 6.20077322611779e-05), (1, 5.146585186275811e-05), (3, 4.543851790492831e-05), (36, 4.515431349396262e-05), (4, 4.5111126786053365e-05), (45, 4.378115521022932e-05), (31, 4.083428756482129e-05), (30, 4.038178319651274e-05), (40, 3.9826502574684154e-05)]\n",
            "['demon', 'versu', 'man'] [(24, 0.0003036895896564607), (52, 0.0002750082373154774), (21, 0.0002517365542059592), (50, 0.00017520105211775628), (9, 0.00013984680188900155), (27, 0.0001245087423478457), (15, 0.00011323054734921204), (43, 9.288414925440314e-05), (16, 8.75424460719474e-05), (5, 8.511803674689042e-05)]\n",
            "['african', 'queen', 'marriag'] [(9, 0.000164392479118351), (24, 0.0001554001554001554), (21, 0.0001481700992739665), (16, 9.892397083600774e-05), (38, 8.286842099174732e-05), (15, 8.176921599295967e-05), (56, 8.09104108482309e-05), (41, 7.905910740822987e-05), (5, 7.645984221684063e-05), (57, 7.302758851163056e-05)]\n",
            "['zulu', 'king'] [(21, 0.0005084255095805286), (9, 0.00026030789905401514), (52, 0.00024525555351794746), (27, 8.913794714125826e-05), (16, 7.72926879333279e-05), (56, 7.572454538184141e-05), (5, 7.298533975920007e-05), (23, 7.294936250195163e-05), (11, 6.807748790814525e-05), (41, 6.360661085228114e-05)]\n",
            "    Score: 0 Feedback: 0/5 Correct. Accuracy: 0.000000\n"
          ]
        }
      ],
      "source": [
        "# Exercice 4. Similarité cosinus\n",
        "def rank_retrieve(self, query):\n",
        "    \"\"\"\n",
        "    A partir d'une requête (une liste de mots), retourne une liste de documents (classés par docID) et de scores pour la requête en appliquant la simalirité cosinus.\n",
        "\n",
        "    Dans l'exemple ci-dessous. C'est la mesure de Jaccard qui est utilisée.\n",
        "    \"\"\"\n",
        "    scores = [0.0 for _ in range(len(self.docs))]\n",
        "\n",
        "    query_set = set(query)\n",
        "\n",
        "    relevantDocs = set()\n",
        "\n",
        "    for d in range(len(self.docs)):\n",
        "        for q in query:\n",
        "          occ = self.docs[d].count(q)\n",
        "          tf = 1+log10(occ) if occ > 0 else 0\n",
        "          scores[d] += tf\n",
        "\n",
        "    for d in range(len(self.docs)):\n",
        "      scores[d] = scores[d]/len(self.docs[d])\n",
        "    # Tri des scores\n",
        "    ranking = [idx for idx, sim in sorted(enumerate(scores),\n",
        "                                        key=lambda pair: pair[1], reverse=True)]\n",
        "    results = []\n",
        "    for i in range(10):\n",
        "        results.append((ranking[i], scores[ranking[i]]))\n",
        "\n",
        "    print(query, results)\n",
        "    return results\n",
        "\n",
        "# Ne pas modifier les lignes ci-dessous\n",
        "IRSystem.rank_retrieve = rank_retrieve\n",
        "run_tests(ir_system, part=3)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}