{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quentin-arzalier/tp-information-retrieval-with-llm-student-version/blob/main/1-Recherche%20d'information%20classique.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugX7FayQ90_A"
      },
      "source": [
        "# Partie 1. - Recherche d'Information classique\n",
        "\n",
        "Dans cette partie, nous allons mettre en oeuvre des principes et modèles classiques de Recherche d'Information. Le jeu de donnée est une collection de livres au format texte (.txt) de Henry Rider Haggard. Jetez un oeil à ces documents dans le dossier _data_.\n",
        "\n",
        "En sortie de ce module, vous serez capable de :\n",
        "\n",
        "- Construire un index inversé ;\n",
        "- Effectuer des requêtes simples selon le modèle booléen :\n",
        "- Calculer la pondération des termes selon la méthode TF-IDF ;\n",
        "- Mettre en oeuvre le modèle vectoriel de recherche d'information et y appliquer des requêtes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw1BGZgY-tvZ"
      },
      "source": [
        "### Import des bibliothèques logicielles et configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOeUX1DuBqQL"
      },
      "source": [
        "Les lignes suivantes permettent d'instancier un objet la classe `IRSystem` représentant notre moteur de recherche et de charger les données en RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-oi-56kBqQL",
        "outputId": "0f0b3357-8f82-4cfc-aea3-31bded747834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'tp-information-retrieval-with-llm-student-version'...\n",
            "remote: Enumerating objects: 2253, done.\u001b[K\n",
            "remote: Counting objects: 100% (41/41), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 2253 (delta 24), reused 29 (delta 15), pack-reused 2212\u001b[K\n",
            "Receiving objects: 100% (2253/2253), 13.96 MiB | 16.22 MiB/s, done.\n",
            "Resolving deltas: 100% (26/26), done.\n",
            "/content/tp-information-retrieval-with-llm-student-version/tp-information-retrieval-with-llm-student-version\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Vérifie si le code est exécuté sur Google Colab\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "    # Commandes à exécuter uniquement sur Google Colab\n",
        "    !git clone https://github.com/quentin-arzalier/tp-information-retrieval-with-llm-student-version.git\n",
        "    %cd tp-information-retrieval-with-llm-student-version\n",
        "else:\n",
        "    # Commandes à exécuter si ce n'est pas sur Google Colab\n",
        "    print(\"Pas sur Google Colab, ces commandes ne seront pas exécutées.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rMp8JVGBqQM"
      },
      "source": [
        "#### Chargement des données\n",
        "\n",
        "Les lignes ci-dessous permettent de charger les données qui sont un ensemble de 60 livres au format texte (.txt) d'[Henry Rider Haggard ](https://fr.wikipedia.org/wiki/Henry_Rider_Haggard).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZNWPECSs9wgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e4c2f2f-1b3a-4388-e8ec-c2accae52d15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading in documents...\n",
            "Stemming Documents...\n",
            "The Ivory Child 2841.txt\n",
            "    Doc 1 of 60: The Ivory Child\n",
            "Eric Brighteyes 2721.txt\n",
            "    Doc 2 of 60: Eric Brighteyes\n",
            "Red Eve 3094.txt\n",
            "    Doc 3 of 60: Red Eve\n",
            "Jess 5898.txt\n",
            "    Doc 4 of 60: Jess\n",
            "Stories by English Authors Africa (Selected by Scribners) 1980.txt\n",
            "    Doc 5 of 60: Stories by English Authors Africa (Selected by Scribners)\n",
            "Love Eternal 3709.txt\n",
            "    Doc 6 of 60: Love Eternal\n",
            "Child of Storm 1711.txt\n",
            "    Doc 7 of 60: Child of Storm\n",
            "Marie An Episode in The Life of the late Allan Quatermain 1690.txt\n",
            "    Doc 8 of 60: Marie An Episode in The Life of the late Allan Quatermain\n",
            "The Wizard 2893.txt\n",
            "    Doc 9 of 60: The Wizard\n",
            "Ayesha, the Return of She 5228.txt\n",
            "    Doc 10 of 60: Ayesha, the Return of She\n",
            "Lysbeth, a Tale of the Dutch 5754.txt\n",
            "    Doc 11 of 60: Lysbeth, a Tale of the Dutch\n",
            "The Wanderer's Necklace 3097.txt\n",
            "    Doc 12 of 60: The Wanderer's Necklace\n",
            "The Lady of Blossholme 3813.txt\n",
            "    Doc 13 of 60: The Lady of Blossholme\n",
            "Black Heart and White Heart 2842.txt\n",
            "    Doc 14 of 60: Black Heart and White Heart\n",
            "Allan and the Holy Flower 5174.txt\n",
            "    Doc 15 of 60: Allan and the Holy Flower\n",
            "She and Allan 5745.txt\n",
            "    Doc 16 of 60: She and Allan\n",
            "The Virgin of the Sun 3153.txt\n",
            "    Doc 17 of 60: The Virgin of the Sun\n",
            "Beatrice 3096.txt\n",
            "    Doc 18 of 60: Beatrice\n",
            "Elissa 2855.txt\n",
            "    Doc 19 of 60: Elissa\n",
            "Moon of Israel 2856.txt\n",
            "    Doc 20 of 60: Moon of Israel\n",
            "A Winter Pilgrimage (1901) 0600121.txt\n",
            "    Doc 21 of 60: A Winter Pilgrimage (1901)\n",
            "Morning Star 2722.txt\n",
            "    Doc 22 of 60: Morning Star\n",
            "Queen Sheba's Ring 2602.txt\n",
            "    Doc 23 of 60: Queen Sheba's Ring\n",
            "Fair Margaret 9780.txt\n",
            "    Doc 24 of 60: Fair Margaret\n",
            "Benita, an African romance 2761.txt\n",
            "    Doc 25 of 60: Benita, an African romance\n",
            "When the World Shook 0.txt\n",
            "    Doc 26 of 60: When the World Shook\n",
            "She 3155.txt\n",
            "    Doc 27 of 60: She\n",
            "The Witch's Head (1884) 0500791.txt\n",
            "    Doc 28 of 60: The Witch's Head (1884)\n",
            "Colonel Quaritch, V.C. A Tale of Country Life 11882.txt\n",
            "    Doc 29 of 60: Colonel Quaritch, V.C. A Tale of Country Life\n",
            "The Ghost Kings 8184.txt\n",
            "    Doc 30 of 60: The Ghost Kings\n",
            "Heu-Heu (1924) 0200191.txt\n",
            "    Doc 31 of 60: Heu-Heu (1924)\n",
            "Montezuma's Daughter 1848.txt\n",
            "    Doc 32 of 60: Montezuma's Daughter\n",
            "Allan Quatermain 711.txt\n",
            "    Doc 33 of 60: Allan Quatermain\n",
            "Hunter Quatermain's Story 2728.txt\n",
            "    Doc 34 of 60: Hunter Quatermain's Story\n",
            "A Yellow God an Idol of Africa 2857.txt\n",
            "    Doc 35 of 60: A Yellow God an Idol of Africa\n",
            "Regeneration 13434.txt\n",
            "    Doc 36 of 60: Regeneration\n",
            "Pearl-Maiden 5175.txt\n",
            "    Doc 37 of 60: Pearl-Maiden\n",
            "Swallow a tale of the great trek 4074.txt\n",
            "    Doc 38 of 60: Swallow a tale of the great trek\n",
            "Maiwa's Revenge 2713.txt\n",
            "    Doc 39 of 60: Maiwa's Revenge\n",
            "Doctor Therne 5764.txt\n",
            "    Doc 40 of 60: Doctor Therne\n",
            "Allan's Wife 2727.txt\n",
            "    Doc 41 of 60: Allan's Wife\n",
            "Long Odds 1918.txt\n",
            "    Doc 42 of 60: Long Odds\n",
            "Dawn 10892.txt\n",
            "    Doc 43 of 60: Dawn\n",
            "Cleopatra 2769.txt\n",
            "    Doc 44 of 60: Cleopatra\n",
            "The People of the Mist 6769.txt\n",
            "    Doc 45 of 60: The People of the Mist\n",
            "The Mahatma and the Hare 2764.txt\n",
            "    Doc 46 of 60: The Mahatma and the Hare\n",
            "Queen of the Dawn (1925) 0200381.txt\n",
            "    Doc 47 of 60: Queen of the Dawn (1925)\n",
            "Cetywayo and his White Neighbours 0.txt\n",
            "    Doc 48 of 60: Cetywayo and his White Neighbours\n",
            "Finished 1724.txt\n",
            "    Doc 49 of 60: Finished\n",
            "Stella Fregelius 6051.txt\n",
            "    Doc 50 of 60: Stella Fregelius\n",
            "King Solomon's Mines 2166.txt\n",
            "    Doc 51 of 60: King Solomon's Mines\n",
            "The Ancient Allan 5746.txt\n",
            "    Doc 52 of 60: The Ancient Allan\n",
            "Mr. Meeson's Will 11913.txt\n",
            "    Doc 53 of 60: Mr. Meeson's Will\n",
            "Wisdom's Daughter (1923) 0200181.txt\n",
            "    Doc 54 of 60: Wisdom's Daughter (1923)\n",
            "Nada the Lily 1207.txt\n",
            "    Doc 55 of 60: Nada the Lily\n",
            "The Brethren 2762.txt\n",
            "    Doc 56 of 60: The Brethren\n",
            "The World's Desire 2763.txt\n",
            "    Doc 57 of 60: The World's Desire\n",
            "The Tale of Three Lions 2729.txt\n",
            "    Doc 58 of 60: The Tale of Three Lions\n",
            "Smith and the Pharaohs, and other Tales 6073.txt\n",
            "    Doc 59 of 60: Smith and the Pharaohs, and other Tales\n",
            "Allan and the Ice Gods (1927) 0200201.txt\n",
            "    Doc 60 of 60: Allan and the Ice Gods (1927)\n"
          ]
        }
      ],
      "source": [
        "from classic_ir.IRSystem import *\n",
        "\n",
        "# !rm -rf ./data/RiderHaggard/stemmed\n",
        "ir_system = IRSystem()\n",
        "ir_system.read_data('./data/RiderHaggard') # chargement des données et prétraitement des documents (stemming)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4n3kuGaBqQN"
      },
      "source": [
        "### Exercice 1. - Construction de l'index inversé\n",
        "\n",
        "Ce premier exercice a pour objectif de construire l'index inversé non positionnel. L'attribut `self.inverted_index` est un tableau associatif qui associe une liste d'entiers (docIDs) à un mot (word).\n",
        "\n",
        "Documentation ici https://docs.python.org/3/library/collections.html#collections.defaultdict.\n",
        "\n",
        "Exercice : modifier la fonction `index` pour calculer l'index inversé.\n",
        "\n",
        "Le résultat ci-dessous indique que vous avez réussi.\n",
        "```\n",
        "===== Running tests =====\n",
        "Inverted Index Test\n",
        "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzLzgP0SBqQO",
        "outputId": "cfcca3da-0a13-4345-8b6f-9c574ce9fd68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexing...\n",
            "===== Running tests =====\n",
            "Inverted Index Test\n",
            "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n"
          ]
        }
      ],
      "source": [
        "# Exercice 1. Indexation\n",
        "\n",
        "def index(self):\n",
        "    \"\"\"\n",
        "    Construit l'index inversé et le stocke dans self.inverted_index.\n",
        "\n",
        "    Dans le code ci-dessous, seul le dictionnaire des tokens est construit. Les postings lists sont vides.\n",
        "    \"\"\"\n",
        "    print(\"Indexing...\")\n",
        "    self.tf = defaultdict(Counter) # tf est un dictionnaire qui à un document associe un Counter de mots.\n",
        "    inverted_index = defaultdict(list) # inverted_index est un dictionnaire qui à un mot associe une liste de documents.\n",
        "    for i in range (len(self.docs)):\n",
        "        for word in self.docs[i]:\n",
        "            if (not i in inverted_index[word]):\n",
        "              inverted_index[word].append(i)\n",
        "\n",
        "    self.inverted_index = inverted_index\n",
        "\n",
        "# Ne pas modifier les lignes ci-dessous\n",
        "IRSystem.index = index\n",
        "ir_system.index()\n",
        "run_tests(ir_system, part=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiskwodfBqQO"
      },
      "source": [
        "### Exercice 2. - Recherche booléenne\n",
        "\n",
        "Ce deuxième exercice a pour objectif d'implémenter la recherche booléenne. La requête `query` est une liste de mots _lemmatisés_ et l'algorithme doit rechercher les documents qui contiennent TOUS ces mots.\n",
        "\n",
        "\n",
        "Exercice : modifier la fonction `boolean_retrieve` pour implémenter la recherche booléenne.\n",
        "\n",
        "\n",
        "Le résultat ci-dessous indique que vous avez réussi.\n",
        "```\n",
        "===== Running tests =====\n",
        "Boolean Retrieval Test\n",
        "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aB9EXqMWBqQO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "358b7a90-5242-4c7a-ac4d-6ae0c4bdc1c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Running tests =====\n",
            "Boolean Retrieval Test\n",
            "46735\n",
            "46735\n",
            "46735\n",
            "46735\n",
            "46735\n",
            "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n"
          ]
        }
      ],
      "source": [
        "# Exercice 2. Recherche booléenne\n",
        "def boolean_retrieve(self, query):\n",
        "    print(len(self.vocab))\n",
        "    \"\"\"\n",
        "    A partir d'une requête sous la forme d'une liste de mots *lemmatisés*,\n",
        "    retourne la liste des documents dans lesquels *tous* ces mots apparaissent (ie une requête AND).\n",
        "    Retourne une liste vide si la requête ne retourne aucun document.\n",
        "\n",
        "    Dans le code ci-dessous, tous les documents répondent.\n",
        "    \"\"\"\n",
        "    docs = list()\n",
        "    for idx, doc in enumerate(self.docs):\n",
        "        test=True\n",
        "        i=0\n",
        "        while(test and i < len(query)):\n",
        "          test=(query[i] in doc)\n",
        "          i+=1\n",
        "        if (test):\n",
        "          docs.append(idx)\n",
        "    return docs\n",
        "\n",
        "# Ne pas modifier les lignes ci-dessous\n",
        "IRSystem.boolean_retrieve = boolean_retrieve\n",
        "run_tests(ir_system, part=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2YjuO6NBqQP"
      },
      "source": [
        "### Exercice 3. - Calcul des poids TF-IDF des termes dans les documents\n",
        "\n",
        "Dans ce troisième exercice, l'objectif est de pré-calculer les poids TF-IDF pour chaque terme dans chaque document. Pour ce faire, appliquer la formule vue en cours. Utiliser le logarithme en base 10.\n",
        "\n",
        "\n",
        "Exercice : modifier la fonction `boolean_retrieve` pour implémenter la recherche booléenne.\n",
        "\n",
        "Le résultat ci-dessous indique que vous avez réussi.\n",
        "```\n",
        "Calculating tf-idf...\n",
        "===== Running tests =====\n",
        "TF-IDF Test\n",
        "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vhp6Tm2BqQP",
        "outputId": "fe1ad8da-bac9-4f39-a114-5a93bc080d6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating tf-idf...\n",
            "0.13337736503241182\n",
            "===== Running tests =====\n",
            "TF-IDF Test\n",
            "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n"
          ]
        }
      ],
      "source": [
        "from math import log10\n",
        "\n",
        "# Exercice 3. calcul des scores tf-idf\n",
        "def compute_tfidf(self):\n",
        "    \"\"\"\n",
        "    Calcule les scores tf-idf pour tous les mots de tous les documents et les stocke dans self.tfidf.\n",
        "\n",
        "    Dans le code ci-dessous, les scores tf-idf sont tous nuls.\n",
        "    \"\"\"\n",
        "    print(\"Calculating tf-idf...\")\n",
        "\n",
        "    N = len(self.docs)  # N = nombre de documents\n",
        "\n",
        "    occurences = defaultdict(Counter)\n",
        "    for i in range(N):\n",
        "      for word in self.docs[i]:\n",
        "        occurences[i][word] += 1\n",
        "\n",
        "\n",
        "    self.tfidf = defaultdict(Counter)\n",
        "    for i in range(N):\n",
        "      for idx, word in enumerate(self.docs[i]):\n",
        "        df = len(self.inverted_index[word])\n",
        "        idf = log10(N/df)\n",
        "        try:\n",
        "            self.tfidf[i][word] = (1+log10(occurences[i][word]))*idf\n",
        "        except ValueError:\n",
        "            self.tfidf[i][word] = 0.\n",
        "\n",
        "    print(self.tfidf[0][self.p.stem(\"separation\")])\n",
        "\n",
        "# Ne pas modifier les lignes ci-dessous\n",
        "IRSystem.compute_tfidf = compute_tfidf\n",
        "ir_system.compute_tfidf()\n",
        "run_tests(ir_system, part=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_N4kqqLBqQP"
      },
      "source": [
        "### Exercice 4. - Calcul de la similarité cosinus.\n",
        "\n",
        "Dans ce troisième exercice, l'objectif est de pré-calculer les poids TF-IDF pour chaque terme dans chaque document. Pour ce faire, appliquer la formule vue en cours en considérant les éléments suivants :\n",
        "- Ne considérer que la mesure TF pour calculer le poids des termes de la requête (on utilise pas IDF).\n",
        "- utiliser le logarithme en base 10.\n",
        "\n",
        "Exercice : modifier la fonction `rank_retrieve` pour implémenter la recherche booléenne.\n",
        "\n",
        "Le résultat ci-dessous indique que vous avez réussi.\n",
        "```\n",
        "===== Running tests =====\n",
        "Cosine Similarity Test\n",
        "    Score: 3 Feedback: 5/5 Correct. Accuracy: 1.000000\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2MLgta7BqQQ",
        "outputId": "64ab4e7d-b69c-4af8-af8a-b2be8e72a0f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Running tests =====\n",
            "Cosine Similarity Test\n",
            "['separ', 'of', 'church', 'and', 'state'] [(38, 2.1419726827862604), (0, 2.136789225653016), (25, 2.1349605665697906), (18, 2.1304899715038657), (14, 2.1225348117779252), (55, 2.1216295185335374), (42, 2.1193139986367604), (13, 2.1109427924451), (46, 2.1085493754179545), (54, 2.1068643980705444)]\n",
            "['priestess', 'ritual', 'sacrific'] [(3, 1.697899637716011), (59, 1.683763952478558), (36, 1.6739563510462148), (40, 1.6650534297328747), (58, 1.558137057486278), (1, 1.4135443130966079), (16, 1.41267430767582), (47, 1.4113812403907187), (48, 1.410300030712635), (42, 1.4022646449955054)]\n",
            "['demon', 'versu', 'man'] [(39, 1.5825325678221915), (15, 1.5394528176145217), (41, 1.4864590856994584), (29, 1.3744513994264207), (16, 1.3609178024320054), (6, 1.3588434813391523), (40, 1.3393022834787274), (20, 1.3313089586145068), (43, 1.3277687910252949), (14, 1.3249841719678033)]\n",
            "['african', 'queen', 'marriag'] [(22, 1.72690201800696), (35, 1.7252339255789992), (55, 1.7203602234581084), (0, 1.7188820316257565), (51, 1.7131844540777301), (45, 1.7128390408068035), (56, 1.7121362462678884), (39, 1.7118002511151889), (20, 1.7107966696152175), (48, 1.7099591273349708)]\n",
            "['zulu', 'king'] [(47, 1.4142135063265293), (3, 1.4142130457660271), (28, 1.4142104176985633), (41, 1.41391944626285), (8, 1.4139129004391222), (20, 1.4138409351124592), (19, 1.4138328099481836), (11, 1.4129384272576346), (9, 1.412133960122286), (2, 1.4107712979090474)]\n",
            "    Score: 0 Feedback: 0/5 Correct. Accuracy: 0.000000\n"
          ]
        }
      ],
      "source": [
        "from math import sqrt\n",
        "\n",
        "# Exercice 4. Similarité cosinus\n",
        "def rank_retrieve(self, query):\n",
        "    \"\"\"\n",
        "    A partir d'une requête (une liste de mots), retourne une liste de documents (classés par docID) et de scores pour la requête en appliquant la simalirité cosinus.\n",
        "\n",
        "    Dans l'exemple ci-dessous. C'est la mesure de Jaccard qui est utilisée.\n",
        "    \"\"\"\n",
        "\n",
        "    query_set = set(query)\n",
        "\n",
        "    relevantDocs = set()\n",
        "\n",
        "    #D'après l'algorithme décrit en slide 58 du cours, ceci devrait donner la bonne réponse :\n",
        "\n",
        "    # 1 float Scores[N] = 0\n",
        "    scores = [0.0 for _ in range(len(self.docs))]\n",
        "    # 2 float Length[N]\n",
        "    length = [0.0 for _ in range(len(self.docs))]\n",
        "\n",
        "    # 4 do calculate wt,q and fetch postings list for t\n",
        "    # 6 for each pair (d, tft,d) in postings list\n",
        "    for d in range(len(self.docs)):\n",
        "        sumTfSquared = 0\n",
        "        # 3 for each query term t\n",
        "        for q in query:\n",
        "          occ = self.docs[d].count(q)\n",
        "          # calcul de wt,d, wt,q toujours 1 car une seule occurence dans query\n",
        "          tf = 1+log10(occ) if occ > 0 else 0\n",
        "          # 6 do Scores[d] += wtd x wtq (encore une fois, wtq = 1)\n",
        "          scores[d] += tf\n",
        "          sumTfSquared += tf**2\n",
        "        # 7 Read the array Length (Racine des tf de la query pour le doc au ²)\n",
        "        length[d] = sqrt(sumTfSquared)\n",
        "\n",
        "    # 8 for each d\n",
        "    for d in range(len(self.docs)):\n",
        "      # 9 do Scores[d] = Scores[d]/Length[d]\n",
        "      scores[d] = scores[d]/length[d] if length[d] > 0 else 0\n",
        "\n",
        "    # Je ne comprends pas pourquoi l'algo tape à côté de la bonne réponse.\n",
        "\n",
        "\n",
        "    # Tri des scores\n",
        "    ranking = [idx for idx, sim in sorted(enumerate(scores),\n",
        "                                        key=lambda pair: pair[1], reverse=True)]\n",
        "    results = []\n",
        "    for i in range(10):\n",
        "        results.append((ranking[i], scores[ranking[i]]))\n",
        "\n",
        "    print(query, results)\n",
        "    return results\n",
        "\n",
        "# Ne pas modifier les lignes ci-dessous\n",
        "IRSystem.rank_retrieve = rank_retrieve\n",
        "run_tests(ir_system, part=3)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}